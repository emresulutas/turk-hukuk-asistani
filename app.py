import streamlit as st
import nest_asyncio
import chromadb
import os
import Stemmer
from llama_index.core import StorageContext, load_index_from_storage, Settings, PromptTemplate
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.retrievers import AutoMergingRetriever, QueryFusionRetriever
from llama_index.retrievers.bm25 import BM25Retriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.gemini import Gemini

# 1. SAYFA AYARLARI
st.set_page_config(page_title="Yerel Hukuk AsistanÄ±", layout="wide", page_icon="âš–ï¸")
st.title("âš–ï¸ Hukuk AsistanÄ± (Gemini 2.5 Flash)")

# Notebook hatasÄ± Ã¶nleyici (Localde de bazen gerekir)
nest_asyncio.apply()

# 2. SÄ°STEMÄ° YÃœKLEME (Cache ile hÄ±zlandÄ±rÄ±lmÄ±ÅŸ)
@st.cache_resource
def load_system():
    # --- A. MODELLER ---
    # Embedding: CPU'da Ã§alÄ±ÅŸsÄ±n (Hafif ve gÃ¼venli)
    embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-m3", device="cpu")
    
    # LLM: Gemini 2.5 Flash
    # BURAYA API ANAHTARINI YAPIÅTIR
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

    if not GOOGLE_API_KEY:
        st.error("âš ï¸ API Key BulunamadÄ±! LÃ¼tfen GOOGLE_API_KEY ortam deÄŸiÅŸkenini ayarlayÄ±n.")
        st.stop()

    llm = Gemini(model="models/gemini-2.5-flash", api_key=api_key)
    
    Settings.llm = llm
    Settings.embed_model = embed_model

    # --- B. VERÄ°TABANI BAÄLANTISI (YEREL) ---
    # KlasÃ¶r yapÄ±na gÃ¶re yollar: './chroma_db' ve './storage'
    base_path = "." 
    
    if not os.path.exists(f"{base_path}/chroma_db"):
        st.error("HATA: 'chroma_db' klasÃ¶rÃ¼ bulunamadÄ±!")
        st.stop()

    db = chromadb.PersistentClient(path=f"{base_path}/chroma_db")
    chroma_collection = db.get_or_create_collection("hukuk_verileri")
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    
    storage_context = StorageContext.from_defaults(
        persist_dir=f"{base_path}/storage", 
        vector_store=vector_store
    )
    index = load_index_from_storage(storage_context)
    
    # --- C. RETRIEVER KURULUMU ---
    nodes = list(storage_context.docstore.docs.values())
    
    # Similarity Top K = 20 (GeniÅŸ Tarama - Gemini iÃ§in ideal)
    base_retriever = index.as_retriever(similarity_top_k=20)
    
    auto_merging_retriever = AutoMergingRetriever(
        base_retriever, 
        storage_context=storage_context
    )
    
    stemmer = Stemmer.Stemmer("turkish")
    bm25_retriever = BM25Retriever.from_defaults(
        nodes=nodes, 
        similarity_top_k=20, 
        stemmer=stemmer, 
        language="turkish"
    )
    
    # --- D. FUSION (Voltran) ---
    retriever_prompt = PromptTemplate(
        "KullanÄ±cÄ±nÄ±n sorusunu veritabanÄ±nda aramak iÃ§in en iyi TÃ¼rkÃ§e arama cÃ¼mlesini yaz.\n"
        "Soru: {query}\nArama CÃ¼mlesi:"
    )
    
    fusion_retriever = QueryFusionRetriever(
        [auto_merging_retriever, bm25_retriever],
        similarity_top_k=20,
        num_queries=3, # 3 farklÄ± aÃ§Ä±dan arasÄ±n
        mode="reciprocal_rerank",
        use_async=True,
        verbose=True,
        query_gen_prompt=retriever_prompt
    )
    
    # --- E. CEVAP MOTORU (Avukat Prompt) ---
    qa_prompt = PromptTemplate(
        "Sen uzman bir TÃ¼rk hukukÃ§ususun. AÅŸaÄŸÄ±daki yasal metinleri analiz et.\n"
        "---------------------\n{context_str}\n---------------------\n"
        "KURALLAR:\n"
        "1. Sadece yukarÄ±daki metne sadÄ±k kal.\n"
        "2. Asla kafandan baÅŸlÄ±k veya iÃ§erik uydurma.\n"
        "3. Ä°lgili maddeyi, fÄ±kralarÄ± ve bentleri eksiksiz ve olduÄŸu gibi aktar.\n"
        "4. EÄŸer madde parÃ§alara ayrÄ±lmÄ±ÅŸsa, hepsini birleÅŸtir ve bÃ¼tÃ¼n halini sun.\n"
        "Soru: {query_str}\n"
        "Cevap:"
    )
    
    return RetrieverQueryEngine.from_args(
        retriever=fusion_retriever,
        llm=llm,
        text_qa_template=qa_prompt
    )

# 3. BAÅLATMA
with st.spinner("Sistem HazÄ±rlanÄ±yor... (Ä°lk aÃ§Ä±lÄ±ÅŸ birkaÃ§ saniye sÃ¼rebilir)"):
    try:
        query_engine = load_system()
        st.success("Sistem HazÄ±r! ğŸš€")
    except Exception as e:
        st.error(f"Sistem baÅŸlatÄ±lamadÄ±: {e}")
        st.stop()

# 4. SOHBET ARAYÃœZÃœ
if "messages" not in st.session_state:
    st.session_state.messages = []

# GeÃ§miÅŸ mesajlarÄ± ekrana yaz
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Yeni soru giriÅŸi
if prompt := st.chat_input("Hukuki sorunuzu buraya yazÄ±n..."):
    # KullanÄ±cÄ± mesajÄ±nÄ± gÃ¶ster ve kaydet
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Asistan cevabÄ±nÄ± Ã¼ret
    with st.chat_message("assistant"):
        with st.spinner("Mevzuat taranÄ±yor..."):
            try:
                response = query_engine.query(prompt)
                st.markdown(str(response))
                st.session_state.messages.append({"role": "assistant", "content": str(response)})
                
                # KaynaklarÄ± gÃ¶ster (Opsiyonel)
                with st.expander("ğŸ“š Kaynak Belgeleri Ä°ncele"):
                    # Skor sÄ±rasÄ±na gÃ¶re ilk 5 kaynaÄŸÄ± gÃ¶sterelim
                    for node in response.source_nodes[:5]:
                        dosya_adi = node.metadata.get('file_name', 'Bilinmiyor')
                        skor = node.score
                        st.write(f"**Dosya:** {dosya_adi} (Alaka: {skor:.2f})")
                        st.caption(node.text[:300] + "...") # Ä°lk 300 karakter
                        st.divider()
                        
            except Exception as e:
                st.error(f"Bir hata oluÅŸtu: {e}")